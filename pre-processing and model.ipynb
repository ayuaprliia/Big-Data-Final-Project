{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4256a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (2.3.0)\n",
      "Collecting tabulate\n",
      "  Obtaining dependency information for tabulate from https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl.metadata\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1f25bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\KULIAH\\SEMESTER 6\\Big Data\\UAS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c110ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"arxiv_db\"]\n",
    "collection = db[\"papers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124a848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dokumen dalam MongoDB: 16799\n"
     ]
    }
   ],
   "source": [
    "total_docs = collection.count_documents({})\n",
    "print(f\"Total dokumen dalam MongoDB: {total_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d53b59e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SAMPLE DATA:\n",
      "Struktur dokumen:\n",
      "  _id: 684134f79cfee88b7605bd0a\n",
      "  id: http://arxiv.org/abs/2001.12004v2\n",
      "  title: Neural MMO v1.3: A Massively Multiagent Game Environment for Training and Evaluating Neural Networks\n",
      "  authors: Joseph Suarez, Yilun Du, Igor Mordatch, Phillip Isola\n",
      "  summary: Progress in multiagent intelligence research is fundamentally limited by the\n",
      "number and quality of e...\n",
      "  published: 2020-01-31\n",
      "  updated: 2020-04-17\n",
      "  primary_category: cs.LG\n",
      "  categories: cs.LG, cs.AI, cs.MA, stat.ML\n",
      "  pdf_url: http://arxiv.org/pdf/2001.12004v2\n",
      "  is_english: True\n",
      "  combined_text: \n",
      "  is_processed: False\n",
      "  processed_summary: \n",
      "  processed_title: \n",
      "  text_length: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SAMPLE DATA:\")\n",
    "sample_doc = collection.find_one({})\n",
    "if sample_doc:\n",
    "    print(\"Struktur dokumen:\")\n",
    "    for key, value in sample_doc.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\" Tidak ada data dalam collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e9e360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STATISTIK KATEGORI:\n",
      "Top 10 kategori:\n",
      "  cs.CV: 2466 papers\n",
      "  cs.LG: 1968 papers\n",
      "  eess.IV: 1072 papers\n",
      "  cs.SE: 976 papers\n",
      "  cs.CR: 970 papers\n",
      "  cs.DS: 854 papers\n",
      "  cs.DB: 852 papers\n",
      "  eess.AS: 788 papers\n",
      "  eess.SY: 764 papers\n",
      "  cs.GT: 741 papers\n"
     ]
    }
   ],
   "source": [
    "# Statistik kategori\n",
    "print(\"\\n STATISTIK KATEGORI:\")\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$primary_category\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"count\": -1}},\n",
    "    {\"$limit\": 10}\n",
    "]\n",
    "top_categories = list(collection.aggregate(pipeline))\n",
    "print(\"Top 10 kategori:\")\n",
    "for cat in top_categories:\n",
    "    print(f\"  {cat['_id']}: {cat['count']} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "645e0e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 3 sample documents...\n"
     ]
    }
   ],
   "source": [
    "class MongoDBPreprocessor:\n",
    "    def __init__(self, collection):\n",
    "        self.collection = collection\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocessing text for BERT model\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs dan email\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        words = text.split()\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def process_sample(self, limit=5):\n",
    "        \"\"\"Process sample data for testing\"\"\"\n",
    "        print(f\" Processing {limit} sample documents...\")\n",
    "        \n",
    "        cursor = self.collection.find({}).limit(limit)\n",
    "        results = []\n",
    "        \n",
    "        for doc in cursor:\n",
    "            original_title = doc.get('title', '')\n",
    "            original_summary = doc.get('summary', '')\n",
    "            \n",
    "            processed_title = self.preprocess_text(original_title)\n",
    "            processed_summary = self.preprocess_text(original_summary)\n",
    "            combined_text = f\"{processed_title} {processed_summary}\".strip()\n",
    "            \n",
    "            result = {\n",
    "                '_id': doc['_id'],\n",
    "                'original_title': original_title,\n",
    "                'processed_title': processed_title,\n",
    "                'original_summary': original_summary[:200] + \"...\" if len(original_summary) > 200 else original_summary,\n",
    "                'processed_summary': processed_summary[:200] + \"...\" if len(processed_summary) > 200 else processed_summary,\n",
    "                'combined_text': combined_text[:300] + \"...\" if len(combined_text) > 300 else combined_text,\n",
    "                'text_length': len(combined_text.split())\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Jalankan preprocessing sample\n",
    "preprocessor = MongoDBPreprocessor(collection)\n",
    "sample_results = preprocessor.process_sample(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b772e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HASIL PREPROCESSING SAMPLE:\n",
      "\n",
      "--- DOKUMEN 1 ---\n",
      "ID: 684134f79cfee88b7605bd0a\n",
      "Original Title: Neural MMO v1.3: A Massively Multiagent Game Environment for Training and Evaluating Neural Networks\n",
      "Processed Title: neural mmo v1 3 a massively multiagent game environment for training and evaluating neural networks\n",
      "Original Summary: Progress in multiagent intelligence research is fundamentally limited by the\n",
      "number and quality of environments available for study. In recent years,\n",
      "simulated games have become a dominant research pl...\n",
      "Processed Summary: progress in multiagent intelligence research is fundamentally limited by the number and quality of environments available for study in recent years simulated games have become a dominant research plat...\n",
      "Combined Text Length: 165 words\n",
      "--------------------------------------------------\n",
      "\n",
      "--- DOKUMEN 2 ---\n",
      "ID: 684134f79cfee88b7605bd0b\n",
      "Original Title: Deontological Ethics By Monotonicity Shape Constraints\n",
      "Processed Title: deontological ethics by monotonicity shape constraints\n",
      "Original Summary: We demonstrate how easy it is for modern machine-learned systems to violate\n",
      "common deontological ethical principles and social norms such as \"favor the\n",
      "less fortunate,\" and \"do not penalize good attri...\n",
      "Processed Summary: we demonstrate how easy it is for modern machine learned systems to violate common deontological ethical principles and social norms such as favor the less fortunate and do not penalize good attribute...\n",
      "Combined Text Length: 123 words\n",
      "--------------------------------------------------\n",
      "\n",
      "--- DOKUMEN 3 ---\n",
      "ID: 684134f79cfee88b7605bd0c\n",
      "Original Title: Pretrained Transformers for Simple Question Answering over Knowledge Graphs\n",
      "Processed Title: pretrained transformers for simple question answering over knowledge graphs\n",
      "Original Summary: Answering simple questions over knowledge graphs is a well-studied problem in\n",
      "question answering. Previous approaches for this task built on recurrent and\n",
      "convolutional neural network based architectu...\n",
      "Processed Summary: answering simple questions over knowledge graphs is a well studied problem in question answering previous approaches for this task built on recurrent and convolutional neural network based architectur...\n",
      "Combined Text Length: 89 words\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\" HASIL PREPROCESSING SAMPLE:\")\n",
    "for i, result in enumerate(sample_results, 1):\n",
    "    print(f\"\\n--- DOKUMEN {i} ---\")\n",
    "    print(f\"ID: {result['_id']}\")\n",
    "    print(f\"Original Title: {result['original_title']}\")\n",
    "    print(f\"Processed Title: {result['processed_title']}\")\n",
    "    print(f\"Original Summary: {result['original_summary']}\")\n",
    "    print(f\"Processed Summary: {result['processed_summary']}\")\n",
    "    print(f\"Combined Text Length: {result['text_length']} words\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c4e2b",
   "metadata": {},
   "source": [
    "## PRE=PROCESS SELURUH DOKUMEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f710d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoDBPreprocessor:\n",
    "    def __init__(self, collection):\n",
    "        self.collection = collection\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocessing minimal untuk BERT tanpa stopword removal/lemmatisasi\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # 1. Remove URLs dan email\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # 2. Remove special characters (tapi pertahankan tanda baca dasar)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', ' ', text)\n",
    "        \n",
    "        # 3. Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def process_all_documents(self, batch_size=500):\n",
    "        \"\"\"Preprocess seluruh dokumen dan simpan ke MongoDB\"\"\"\n",
    "        total_docs = self.collection.count_documents({})\n",
    "        print(f\"Memulai preprocessing untuk {total_docs} dokumen...\")\n",
    "        \n",
    "        # Setup progress bar\n",
    "        pbar = tqdm(total=total_docs)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_docs, batch_size):\n",
    "            batch = list(self.collection.find({}).skip(i).limit(batch_size))\n",
    "            \n",
    "            bulk_operations = []\n",
    "            for doc in batch:\n",
    "                # Preprocess title dan summary\n",
    "                processed_title = self.preprocess_text(doc.get('title', ''))\n",
    "                processed_summary = self.preprocess_text(doc.get('summary', ''))\n",
    "                combined_text = f\"{processed_title} {processed_summary}\".strip()\n",
    "                \n",
    "                # Prepare update operation\n",
    "                bulk_operations.append(\n",
    "                    pymongo.UpdateOne(\n",
    "                        {'_id': doc['_id']},\n",
    "                        {'$set': {\n",
    "                            'processed_title': processed_title,\n",
    "                            'processed_summary': processed_summary,\n",
    "                            'combined_text': combined_text,\n",
    "                            'text_length': len(combined_text.split()),\n",
    "                            'is_processed': True\n",
    "                        }}\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            # Execute bulk write\n",
    "            if bulk_operations:\n",
    "                self.collection.bulk_write(bulk_operations)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(len(batch))\n",
    "        \n",
    "        pbar.close()\n",
    "        print(f\"Preprocessing selesai. Total {total_docs} dokumen diproses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dee2fce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai preprocessing untuk 16799 dokumen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16799/16799 [00:03<00:00, 5196.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing selesai. Total 16799 dokumen diproses.\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Type      | Title                                                                               | Summary                                                                                                 |   Length |\n",
      "+===========+=====================================================================================+=========================================================================================================+==========+\n",
      "| Original  | Neural MMO v1.3: A Massively Multiagent Game Environment for Training and Evalua... | Progress in multiagent intelligence research is fundamentally limited by the                            |      147 |\n",
      "|           |                                                                                     | number and quality of e...                                                                              |          |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Processed | Neural MMO v1.3 A Massively Multiagent Game Environment for Training and Evaluat... | Progress in multiagent intelligence research is fundamentally limited by the number and quality of e... |      167 |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Original  | Deontological Ethics By Monotonicity Shape Constraints                              | We demonstrate how easy it is for modern machine-learned systems to violate                             |      112 |\n",
      "|           |                                                                                     | common deontological eth...                                                                             |          |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Processed | Deontological Ethics By Monotonicity Shape Constraints                              | We demonstrate how easy it is for modern machine learned systems to violate common deontological eth... |      123 |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Original  | Pretrained Transformers for Simple Question Answering over Knowledge Graphs         | Answering simple questions over knowledge graphs is a well-studied problem in                           |       77 |\n",
      "|           |                                                                                     | question answering. Pr...                                                                               |          |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Processed | Pretrained Transformers for Simple Question Answering over Knowledge Graphs         | Answering simple questions over knowledge graphs is a well studied problem in question answering. Pr... |       88 |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Koneksi ke MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"arxiv_db\"]\n",
    "collection = db[\"papers\"]\n",
    "\n",
    "# Inisialisasi field untuk data yang sudah diproses\n",
    "collection.update_many(\n",
    "    {},\n",
    "    {'$set': {\n",
    "        'is_processed': False,\n",
    "        'processed_title': '',\n",
    "        'processed_summary': '',\n",
    "        'combined_text': '',\n",
    "        'text_length': 0\n",
    "    }},\n",
    "    upsert=False\n",
    ")\n",
    "\n",
    "# Jalankan preprocessing\n",
    "preprocessor = MongoDBPreprocessor(collection)\n",
    "preprocessor.process_all_documents()\n",
    "\n",
    "\n",
    "def display_as_dataframe(num_samples=3):\n",
    "    samples = list(collection.find({'is_processed': True}).limit(num_samples))\n",
    "    \n",
    "    data = []\n",
    "    for sample in samples:\n",
    "        data.append({\n",
    "            'Type': 'Original',\n",
    "            'Title': sample.get('title', '')[:80] + \"...\" if len(sample.get('title', '')) > 80 else sample.get('title', ''),\n",
    "            'Summary': sample.get('summary', '')[:100] + \"...\" if len(sample.get('summary', '')) > 100 else sample.get('summary', ''),\n",
    "            'Length': len(sample.get('summary', '').split())\n",
    "        })\n",
    "        data.append({\n",
    "            'Type': 'Processed', \n",
    "            'Title': sample.get('processed_title', '')[:80] + \"...\" if len(sample.get('processed_title', '')) > 80 else sample.get('processed_title', ''),\n",
    "            'Summary': sample.get('processed_summary', '')[:100] + \"...\" if len(sample.get('processed_summary', '')) > 100 else sample.get('processed_summary', ''),\n",
    "            'Length': sample.get('text_length', 0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    pd.set_option('display.max_colwidth', 60)\n",
    "    print(df.to_markdown(tablefmt=\"grid\", index=False))\n",
    "\n",
    "display_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69591967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 525/525 [08:31<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16799 embedding berhasil disimpan.\n"
     ]
    }
   ],
   "source": [
    "# Load model BERT ringan untuk efisiensi\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Ambil dokumen yang sudah diproses tapi belum punya embedding\n",
    "cursor = collection.find({\n",
    "    \"is_processed\": True,\n",
    "    \"embedding\": {\"$exists\": False}\n",
    "})\n",
    "\n",
    "batch = list(cursor)\n",
    "texts = [doc['combined_text'] for doc in batch]\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Simpan ke MongoDB\n",
    "ops = []\n",
    "for doc, embed in zip(batch, embeddings):\n",
    "    ops.append(UpdateOne(\n",
    "        {\"_id\": doc[\"_id\"]},\n",
    "        {\"$set\": {\"embedding\": embed.tolist()}}\n",
    "    ))\n",
    "\n",
    "if ops:\n",
    "    collection.bulk_write(ops)\n",
    "    print(f\"{len(ops)} embedding berhasil disimpan.\")\n",
    "else:\n",
    "    print(\"Tidak ada dokumen baru untuk diproses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "197965f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DOKUMEN #1 =====\n",
      "Title             : Neural MMO v1.3: A Massively Multiagent Game Environment for Training and Evaluating Neural Networks\n",
      "Kategori         : cs.LG\n",
      "Published         : 2020-01-31\n",
      "PDF URL           : http://arxiv.org/pdf/2001.12004v2\n",
      "Processed Title   : Neural MMO v1.3 A Massively Multiagent Game Environment for Training and Evaluating Neural Networks\n",
      "Processed Summary : Progress in multiagent intelligence research is fundamentally limited by the number and quality of environments available for study. In recent years, simulated games have become a dominant research pl...\n",
      "Combined Text     : Neural MMO v1.3 A Massively Multiagent Game Environment for Training and Evaluating Neural Networks Progress in multiagent intelligence research is fundamentally limited by the number and quality of e...\n",
      "Text Length       : 167 words\n",
      "Embedding Sample  : [-0.02622828260064125, -0.05907943844795227, -0.013319294899702072, -0.021589957177639008, 0.011467973701655865] ... (dimensi: 5 total)\n",
      "======================================================================\n",
      "\n",
      "===== DOKUMEN #2 =====\n",
      "Title             : Deontological Ethics By Monotonicity Shape Constraints\n",
      "Kategori         : cs.LG\n",
      "Published         : 2020-01-31\n",
      "PDF URL           : http://arxiv.org/pdf/2001.11990v2\n",
      "Processed Title   : Deontological Ethics By Monotonicity Shape Constraints\n",
      "Processed Summary : We demonstrate how easy it is for modern machine learned systems to violate common deontological ethical principles and social norms such as favor the less fortunate, and do not penalize good attribut...\n",
      "Combined Text     : Deontological Ethics By Monotonicity Shape Constraints We demonstrate how easy it is for modern machine learned systems to violate common deontological ethical principles and social norms such as favo...\n",
      "Text Length       : 123 words\n",
      "Embedding Sample  : [-0.055357035249471664, 0.007413987535983324, -0.00033667549723759294, -0.006227720528841019, -0.05040818080306053] ... (dimensi: 5 total)\n",
      "======================================================================\n",
      "\n",
      "===== DOKUMEN #3 =====\n",
      "Title             : Pretrained Transformers for Simple Question Answering over Knowledge Graphs\n",
      "Kategori         : cs.CL\n",
      "Published         : 2020-01-31\n",
      "PDF URL           : http://arxiv.org/pdf/2001.11985v1\n",
      "Processed Title   : Pretrained Transformers for Simple Question Answering over Knowledge Graphs\n",
      "Processed Summary : Answering simple questions over knowledge graphs is a well studied problem in question answering. Previous approaches for this task built on recurrent and convolutional neural network based architectu...\n",
      "Combined Text     : Pretrained Transformers for Simple Question Answering over Knowledge Graphs Answering simple questions over knowledge graphs is a well studied problem in question answering. Previous approaches for th...\n",
      "Text Length       : 88 words\n",
      "Embedding Sample  : [-0.12171585112810135, -0.04430079832673073, 0.07976559549570084, 0.06777589023113251, -0.03756524994969368] ... (dimensi: 5 total)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ambil beberapa dokumen yang sudah punya embedding\n",
    "cursor = collection.find(\n",
    "    {\"embedding\": {\"$exists\": True}},\n",
    "    {\n",
    "        \"title\": 1,\n",
    "        \"processed_title\": 1,\n",
    "        \"processed_summary\": 1,\n",
    "        \"combined_text\": 1,\n",
    "        \"text_length\": 1,\n",
    "        \"embedding\": {\"$slice\": 5}, \n",
    "        \"published\": 1,\n",
    "        \"primary_category\": 1,\n",
    "        \"pdf_url\": 1\n",
    "    }\n",
    ").limit(3)\n",
    "\n",
    "# Format tampilan\n",
    "docs = list(cursor)\n",
    "\n",
    "if not docs:\n",
    "    print(\"Belum ada dokumen dengan embedding.\")\n",
    "else:\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"\\n===== DOKUMEN #{i} =====\")\n",
    "        print(f\"Title             : {doc.get('title', '')}\")\n",
    "        print(f\"Kategori         : {doc.get('primary_category', '')}\")\n",
    "        print(f\"Published         : {doc.get('published', '')}\")\n",
    "        print(f\"PDF URL           : {doc.get('pdf_url', '')}\")\n",
    "        print(f\"Processed Title   : {doc.get('processed_title', '')}\")\n",
    "        print(f\"Processed Summary : {doc.get('processed_summary', '')[:200]}...\")\n",
    "        print(f\"Combined Text     : {doc.get('combined_text', '')[:200]}...\")\n",
    "        print(f\"Text Length       : {doc.get('text_length', 0)} words\")\n",
    "        print(f\"Embedding Sample  : {doc.get('embedding', [])[:5]} ... (dimensi: {len(doc.get('embedding', []))} total)\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "555d5717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Artikel #1 ==\n",
      "Title      : Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future\n",
      "Score      : 0.5899\n",
      "Authors    : Grace W. Lindsay\n",
      "Published  : 2020-01-20\n",
      "Category   : q-bio.NC\n",
      "PDF URL    : http://arxiv.org/pdf/2001.07092v2\n",
      "Summary    : Convolutional neural networks (CNNs) were inspired by early findings in the\n",
      "study of biological vision. They have since become successful tools in computer\n",
      "vision and state-of-the-art models of both neural activity and behavior on\n",
      "visual tasks. This review highlights what, in the context of CNNs, it...\n",
      "\n",
      "\n",
      "== Artikel #2 ==\n",
      "Title      : Research Progress of Convolutional Neural Network and its Application in Object Detection\n",
      "Score      : 0.5846\n",
      "Authors    : Wei Zhang, Zuoxiang Zeng\n",
      "Published  : 2020-07-27\n",
      "Category   : cs.CV\n",
      "PDF URL    : http://arxiv.org/pdf/2007.13284v1\n",
      "Summary    : With the improvement of computer performance and the increase of data volume,\n",
      "the object detection based on convolutional neural network (CNN) has become the\n",
      "main algorithm for object detection. This paper summarizes the research\n",
      "progress of convolutional neural networks and their applications in ob...\n",
      "\n",
      "\n",
      "== Artikel #3 ==\n",
      "Title      : Adaptive Convolution Kernel for Artificial Neural Networks\n",
      "Score      : 0.5723\n",
      "Authors    : F. Boray Tek, İlker Çam, Deniz Karlı\n",
      "Published  : 2020-09-14\n",
      "Category   : cs.CV\n",
      "PDF URL    : http://arxiv.org/pdf/2009.06385v1\n",
      "Summary    : Many deep neural networks are built by using stacked convolutional layers of\n",
      "fixed and single size (often 3$\\times$3) kernels. This paper describes a method\n",
      "for training the size of convolutional kernels to provide varying size kernels\n",
      "in a single layer. The method utilizes a differentiable, and the...\n",
      "\n",
      "\n",
      "== Artikel #4 ==\n",
      "Title      : Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network\n",
      "Score      : 0.5687\n",
      "Authors    : Asmaa Abbas, Mohammed M. Abdelsamea, Mohamed Medhat Gaber\n",
      "Published  : 2020-03-26\n",
      "Category   : eess.IV\n",
      "PDF URL    : http://arxiv.org/pdf/2003.13815v3\n",
      "Summary    : Chest X-ray is the first imaging technique that plays an important role in\n",
      "the diagnosis of COVID-19 disease. Due to the high availability of large-scale\n",
      "annotated image datasets, great success has been achieved using convolutional\n",
      "neural networks (CNNs) for image recognition and classification. How...\n",
      "\n",
      "\n",
      "== Artikel #5 ==\n",
      "Title      : C-Net: A Reliable Convolutional Neural Network for Biomedical Image Classification\n",
      "Score      : 0.5623\n",
      "Authors    : Hosein Barzekar, Zeyun Yu\n",
      "Published  : 2020-10-30\n",
      "Category   : eess.IV\n",
      "PDF URL    : http://arxiv.org/pdf/2011.00081v2\n",
      "Summary    : Cancers are the leading cause of death in many countries. Early diagnosis\n",
      "plays a crucial role in having proper treatment for this debilitating disease.\n",
      "The automated classification of the type of cancer is a challenging task since\n",
      "pathologists must examine a huge number of histopathological images ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def recommend_articles(query, top_k=5):\n",
    "    # Encode query ke embedding\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Ambil semua dokumen yang sudah punya embedding\n",
    "    cursor = collection.find({\"embedding\": {\"$exists\": True}})\n",
    "    \n",
    "    results = []\n",
    "    for doc in cursor:\n",
    "        doc_embedding = np.array(doc[\"embedding\"])\n",
    "        similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "        results.append((doc, similarity))\n",
    "    \n",
    "    # Urutkan berdasarkan similarity\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n== Artikel #{i} ==\")\n",
    "        print(f\"Title      : {doc['title']}\")\n",
    "        print(f\"Score      : {score:.4f}\")\n",
    "        print(f\"Authors    : {doc['authors']}\")\n",
    "        print(f\"Published  : {doc['published']}\")\n",
    "        print(f\"Category   : {doc['primary_category']}\")\n",
    "        print(f\"PDF URL    : {doc['pdf_url']}\")\n",
    "        print(f\"Summary    : {doc['summary'][:300]}...\\n\")\n",
    "\n",
    "# Contoh penggunaan\n",
    "recommend_articles(\"Convolutional Neural Networks for Image Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978fe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
