{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4256a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (2.3.0)\n",
      "Collecting tabulate\n",
      "  Obtaining dependency information for tabulate from https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl.metadata\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\kuliah\\semester 6\\big data\\uas\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f25bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c110ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"arxiv_db\"]\n",
    "collection = db[\"papers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124a848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dokumen dalam MongoDB: 16799\n"
     ]
    }
   ],
   "source": [
    "total_docs = collection.count_documents({})\n",
    "print(f\"Total dokumen dalam MongoDB: {total_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d53b59e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SAMPLE DATA:\n",
      "Struktur dokumen:\n",
      "  _id: 684134f79cfee88b7605bd0a\n",
      "  id: http://arxiv.org/abs/2001.12004v2\n",
      "  title: Neural MMO v1.3: A Massively Multiagent Game Environment for Training and Evaluating Neural Networks\n",
      "  authors: Joseph Suarez, Yilun Du, Igor Mordatch, Phillip Isola\n",
      "  summary: Progress in multiagent intelligence research is fundamentally limited by the\n",
      "number and quality of e...\n",
      "  published: 2020-01-31\n",
      "  updated: 2020-04-17\n",
      "  primary_category: cs.LG\n",
      "  categories: cs.LG, cs.AI, cs.MA, stat.ML\n",
      "  pdf_url: http://arxiv.org/pdf/2001.12004v2\n",
      "  is_english: True\n",
      "  combined_text: \n",
      "  is_processed: False\n",
      "  processed_summary: \n",
      "  processed_title: \n",
      "  text_length: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SAMPLE DATA:\")\n",
    "sample_doc = collection.find_one({})\n",
    "if sample_doc:\n",
    "    print(\"Struktur dokumen:\")\n",
    "    for key, value in sample_doc.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\" Tidak ada data dalam collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e9e360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STATISTIK KATEGORI:\n",
      "Top 10 kategori:\n",
      "  cs.CV: 2466 papers\n",
      "  cs.LG: 1968 papers\n",
      "  eess.IV: 1072 papers\n",
      "  cs.SE: 976 papers\n",
      "  cs.CR: 970 papers\n",
      "  cs.DS: 854 papers\n",
      "  cs.DB: 852 papers\n",
      "  eess.AS: 788 papers\n",
      "  eess.SY: 764 papers\n",
      "  cs.GT: 741 papers\n"
     ]
    }
   ],
   "source": [
    "# Statistik kategori\n",
    "print(\"\\n STATISTIK KATEGORI:\")\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$primary_category\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"count\": -1}},\n",
    "    {\"$limit\": 10}\n",
    "]\n",
    "top_categories = list(collection.aggregate(pipeline))\n",
    "print(\"Top 10 kategori:\")\n",
    "for cat in top_categories:\n",
    "    print(f\"  {cat['_id']}: {cat['count']} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "645e0e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 3 sample documents...\n"
     ]
    }
   ],
   "source": [
    "class MongoDBPreprocessor:\n",
    "    def __init__(self, collection):\n",
    "        self.collection = collection\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocessing text for BERT model\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs dan email\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        words = text.split()\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def process_sample(self, limit=5):\n",
    "        \"\"\"Process sample data for testing\"\"\"\n",
    "        print(f\" Processing {limit} sample documents...\")\n",
    "        \n",
    "        cursor = self.collection.find({}).limit(limit)\n",
    "        results = []\n",
    "        \n",
    "        for doc in cursor:\n",
    "            original_title = doc.get('title', '')\n",
    "            original_summary = doc.get('summary', '')\n",
    "            \n",
    "            processed_title = self.preprocess_text(original_title)\n",
    "            processed_summary = self.preprocess_text(original_summary)\n",
    "            combined_text = f\"{processed_title} {processed_summary}\".strip()\n",
    "            \n",
    "            result = {\n",
    "                '_id': doc['_id'],\n",
    "                'original_title': original_title,\n",
    "                'processed_title': processed_title,\n",
    "                'original_summary': original_summary[:200] + \"...\" if len(original_summary) > 200 else original_summary,\n",
    "                'processed_summary': processed_summary[:200] + \"...\" if len(processed_summary) > 200 else processed_summary,\n",
    "                'combined_text': combined_text[:300] + \"...\" if len(combined_text) > 300 else combined_text,\n",
    "                'text_length': len(combined_text.split())\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Jalankan preprocessing sample\n",
    "preprocessor = MongoDBPreprocessor(collection)\n",
    "sample_results = preprocessor.process_sample(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b772e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HASIL PREPROCESSING SAMPLE:\n",
      "\n",
      "--- DOKUMEN 1 ---\n",
      "ID: 684134f79cfee88b7605bd0a\n",
      "Original Title: Neural MMO v1.3: A Massively Multiagent Game Environment for Training and Evaluating Neural Networks\n",
      "Processed Title: neural mmo v1 3 a massively multiagent game environment for training and evaluating neural networks\n",
      "Original Summary: Progress in multiagent intelligence research is fundamentally limited by the\n",
      "number and quality of environments available for study. In recent years,\n",
      "simulated games have become a dominant research pl...\n",
      "Processed Summary: progress in multiagent intelligence research is fundamentally limited by the number and quality of environments available for study in recent years simulated games have become a dominant research plat...\n",
      "Combined Text Length: 165 words\n",
      "--------------------------------------------------\n",
      "\n",
      "--- DOKUMEN 2 ---\n",
      "ID: 684134f79cfee88b7605bd0b\n",
      "Original Title: Deontological Ethics By Monotonicity Shape Constraints\n",
      "Processed Title: deontological ethics by monotonicity shape constraints\n",
      "Original Summary: We demonstrate how easy it is for modern machine-learned systems to violate\n",
      "common deontological ethical principles and social norms such as \"favor the\n",
      "less fortunate,\" and \"do not penalize good attri...\n",
      "Processed Summary: we demonstrate how easy it is for modern machine learned systems to violate common deontological ethical principles and social norms such as favor the less fortunate and do not penalize good attribute...\n",
      "Combined Text Length: 123 words\n",
      "--------------------------------------------------\n",
      "\n",
      "--- DOKUMEN 3 ---\n",
      "ID: 684134f79cfee88b7605bd0c\n",
      "Original Title: Pretrained Transformers for Simple Question Answering over Knowledge Graphs\n",
      "Processed Title: pretrained transformers for simple question answering over knowledge graphs\n",
      "Original Summary: Answering simple questions over knowledge graphs is a well-studied problem in\n",
      "question answering. Previous approaches for this task built on recurrent and\n",
      "convolutional neural network based architectu...\n",
      "Processed Summary: answering simple questions over knowledge graphs is a well studied problem in question answering previous approaches for this task built on recurrent and convolutional neural network based architectur...\n",
      "Combined Text Length: 89 words\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\" HASIL PREPROCESSING SAMPLE:\")\n",
    "for i, result in enumerate(sample_results, 1):\n",
    "    print(f\"\\n--- DOKUMEN {i} ---\")\n",
    "    print(f\"ID: {result['_id']}\")\n",
    "    print(f\"Original Title: {result['original_title']}\")\n",
    "    print(f\"Processed Title: {result['processed_title']}\")\n",
    "    print(f\"Original Summary: {result['original_summary']}\")\n",
    "    print(f\"Processed Summary: {result['processed_summary']}\")\n",
    "    print(f\"Combined Text Length: {result['text_length']} words\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c4e2b",
   "metadata": {},
   "source": [
    "## PRE=PROCESS SELURUH DOKUMEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f710d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoDBPreprocessor:\n",
    "    def __init__(self, collection):\n",
    "        self.collection = collection\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocessing minimal untuk BERT tanpa stopword removal/lemmatisasi\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # 1. Remove URLs dan email\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # 2. Remove special characters (tapi pertahankan tanda baca dasar)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', ' ', text)\n",
    "        \n",
    "        # 3. Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def process_all_documents(self, batch_size=500):\n",
    "        \"\"\"Preprocess seluruh dokumen dan simpan ke MongoDB\"\"\"\n",
    "        total_docs = self.collection.count_documents({})\n",
    "        print(f\"Memulai preprocessing untuk {total_docs} dokumen...\")\n",
    "        \n",
    "        # Setup progress bar\n",
    "        pbar = tqdm(total=total_docs)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_docs, batch_size):\n",
    "            batch = list(self.collection.find({}).skip(i).limit(batch_size))\n",
    "            \n",
    "            bulk_operations = []\n",
    "            for doc in batch:\n",
    "                # Preprocess title dan summary\n",
    "                processed_title = self.preprocess_text(doc.get('title', ''))\n",
    "                processed_summary = self.preprocess_text(doc.get('summary', ''))\n",
    "                combined_text = f\"{processed_title} {processed_summary}\".strip()\n",
    "                \n",
    "                # Prepare update operation\n",
    "                bulk_operations.append(\n",
    "                    pymongo.UpdateOne(\n",
    "                        {'_id': doc['_id']},\n",
    "                        {'$set': {\n",
    "                            'processed_title': processed_title,\n",
    "                            'processed_summary': processed_summary,\n",
    "                            'combined_text': combined_text,\n",
    "                            'text_length': len(combined_text.split()),\n",
    "                            'is_processed': True\n",
    "                        }}\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            # Execute bulk write\n",
    "            if bulk_operations:\n",
    "                self.collection.bulk_write(bulk_operations)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(len(batch))\n",
    "        \n",
    "        pbar.close()\n",
    "        print(f\"Preprocessing selesai. Total {total_docs} dokumen diproses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dee2fce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai preprocessing untuk 16799 dokumen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16799/16799 [00:03<00:00, 5196.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing selesai. Total 16799 dokumen diproses.\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Type      | Title                                                                               | Summary                                                                                                 |   Length |\n",
      "+===========+=====================================================================================+=========================================================================================================+==========+\n",
      "| Original  | Neural MMO v1.3: A Massively Multiagent Game Environment for Training and Evalua... | Progress in multiagent intelligence research is fundamentally limited by the                            |      147 |\n",
      "|           |                                                                                     | number and quality of e...                                                                              |          |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Processed | Neural MMO v1.3 A Massively Multiagent Game Environment for Training and Evaluat... | Progress in multiagent intelligence research is fundamentally limited by the number and quality of e... |      167 |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Original  | Deontological Ethics By Monotonicity Shape Constraints                              | We demonstrate how easy it is for modern machine-learned systems to violate                             |      112 |\n",
      "|           |                                                                                     | common deontological eth...                                                                             |          |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Processed | Deontological Ethics By Monotonicity Shape Constraints                              | We demonstrate how easy it is for modern machine learned systems to violate common deontological eth... |      123 |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Original  | Pretrained Transformers for Simple Question Answering over Knowledge Graphs         | Answering simple questions over knowledge graphs is a well-studied problem in                           |       77 |\n",
      "|           |                                                                                     | question answering. Pr...                                                                               |          |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n",
      "| Processed | Pretrained Transformers for Simple Question Answering over Knowledge Graphs         | Answering simple questions over knowledge graphs is a well studied problem in question answering. Pr... |       88 |\n",
      "+-----------+-------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Koneksi ke MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"arxiv_db\"]\n",
    "collection = db[\"papers\"]\n",
    "\n",
    "# Inisialisasi field untuk data yang sudah diproses\n",
    "collection.update_many(\n",
    "    {},\n",
    "    {'$set': {\n",
    "        'is_processed': False,\n",
    "        'processed_title': '',\n",
    "        'processed_summary': '',\n",
    "        'combined_text': '',\n",
    "        'text_length': 0\n",
    "    }},\n",
    "    upsert=False\n",
    ")\n",
    "\n",
    "# Jalankan preprocessing\n",
    "preprocessor = MongoDBPreprocessor(collection)\n",
    "preprocessor.process_all_documents()\n",
    "\n",
    "\n",
    "def display_as_dataframe(num_samples=3):\n",
    "    samples = list(collection.find({'is_processed': True}).limit(num_samples))\n",
    "    \n",
    "    data = []\n",
    "    for sample in samples:\n",
    "        data.append({\n",
    "            'Type': 'Original',\n",
    "            'Title': sample.get('title', '')[:80] + \"...\" if len(sample.get('title', '')) > 80 else sample.get('title', ''),\n",
    "            'Summary': sample.get('summary', '')[:100] + \"...\" if len(sample.get('summary', '')) > 100 else sample.get('summary', ''),\n",
    "            'Length': len(sample.get('summary', '').split())\n",
    "        })\n",
    "        data.append({\n",
    "            'Type': 'Processed', \n",
    "            'Title': sample.get('processed_title', '')[:80] + \"...\" if len(sample.get('processed_title', '')) > 80 else sample.get('processed_title', ''),\n",
    "            'Summary': sample.get('processed_summary', '')[:100] + \"...\" if len(sample.get('processed_summary', '')) > 100 else sample.get('processed_summary', ''),\n",
    "            'Length': sample.get('text_length', 0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    pd.set_option('display.max_colwidth', 60)\n",
    "    print(df.to_markdown(tablefmt=\"grid\", index=False))\n",
    "\n",
    "display_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ac675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
